{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello!\n",
    "\n",
    "The following code:\n",
    "\n",
    "1) Counts the number of words that occur only once in a number of Renaissance poems and compares that number to other  literary texts.\n",
    "\n",
    "2) Plots the rank-frequency distribution of word tokens for each work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries I used \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "import pyLDAvis.sklearn\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import scipy\n",
    "import collections\n",
    "\n",
    "from cltk.corpus.utils.importer import CorpusImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads the CLTK pre-trained models for Latin and the LL (Latin Library) corpus\n",
    "\n",
    "my_latin_downloader = CorpusImporter('latin')\n",
    "my_latin_downloader.import_corpus('latin_text_latin_library')\n",
    "my_latin_downloader.import_corpus('latin_models_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes four Latin poems from directories in the downloaded Latin Library corpus and opens them \n",
    "\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "latinlibrary = get_corpus_reader(corpus_name = 'latin_text_latin_library', language = 'latin')\n",
    "files = latinlibrary.fileids()\n",
    "ll = latinlibrary.raw(files)\n",
    "\n",
    "## Loop through the files in the author folders and call the .raw method on the 3 lists\n",
    "\n",
    "# De Rerum Natura \n",
    "dererumnatura_files = [file for file in files if 'lucretius/lucretius' in file]\n",
    "dererumnatura = latinlibrary.raw(dererumnatura_files)\n",
    "# Vergil's Aeneid\n",
    "aeneid_files = [file for file in files if 'vergil/aen' in file]\n",
    "aeneid = latinlibrary.raw(aeneid_files)\n",
    "# Statius' Thebaid \n",
    "thebaid_files = [file for file in files if 'statius/theb' in file]\n",
    "thebaid = latinlibrary.raw(thebaid_files)\n",
    "# Lucan's Pharsalia\n",
    "pharsalia_files = [file for file in files if 'lucan/lucan' in file]\n",
    "pharsalia = latinlibrary.raw(pharsalia_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prepares the CLTK Latin lemmatizer to use on the various Latin texts\n",
    "\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "corpus_importer = CorpusImporter('latin')\n",
    "tokenizer = TokenizeSentence('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/matthewmason/cltk/corpus/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assigns the MP texts to variables. The .csv included data from 10 MP texts\n",
    "\n",
    "baldus = open(\"Baldus1552.txt\", encoding = \"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/matthewmason/cltk/corpus2/neo-Latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neo-Latin poems\n",
    "\n",
    "africa = open(\"africa.txt\", encoding = \"utf-8\").read()\n",
    "christiad = open(\"christiad.txt\", encoding = \"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/matthewmason/cltk/corpus2/italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Italian poems\n",
    "\n",
    "commedia = open(\"commedia.txt\", encoding = \"utf-8\").read()\n",
    "morgante = open(\"morgante.txt\", encoding = \"utf-8\").read()\n",
    "orlando_innamorato = open(\"orlando_innamorato.txt\", encoding = \"utf-8\").read()\n",
    "orlando_furioso = open(\"orlando_furioso.txt\", encoding = \"utf-8\").read()\n",
    "italia_liberata = open(\"italia_liberata.txt\", encoding = \"utf-8\").read()\n",
    "gerusalemme_liberata = open(\"gerusalemme_liberata.txt\", encoding = \"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/matthewmason/cltk/corpus2/english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The English poems, too\n",
    "\n",
    "faerie_queen = open(\"faerie_queen.txt\", encoding = \"utf-8\").read()\n",
    "venus_and_adonis = open(\"venus_and_adonis.txt\", encoding = \"utf-8\").read()\n",
    "paradise_lost = open(\"paradise_lost.txt\", encoding = \"utf-8\").read()\n",
    "finnagans_wake = open(\"finnagans_wake.txt\", encoding = \"utf-8\").read()\n",
    "ulysses = open(\"ulysses.txt\", encoding = \"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stop.latin import STOPS_LIST\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "lemmatizer = BackoffLatinLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This lowers the capital letters  \n",
    "\n",
    "baldus_lowered = baldus.lower()\n",
    "\n",
    "dererumnatura_lowered = dererumnatura.lower()\n",
    "aeneid_lowered = aeneid.lower()\n",
    "thebaid_lowered = thebaid.lower() \n",
    "pharsalia_lowered = pharsalia.lower()\n",
    "\n",
    "africa_lowered = africa.lower()\n",
    "christiad_lowered = christiad.lower() \n",
    "\n",
    "commedia_lowered = commedia.lower()\n",
    "morgante_lowered = morgante.lower()\n",
    "orlando_innamorato_lowered = orlando_innamorato.lower()\n",
    "orlando_furioso_lowered = orlando_furioso.lower()\n",
    "italia_liberata_lowered = italia_liberata.lower()\n",
    "gerusalemme_liberata_lowered = gerusalemme_liberata.lower()\n",
    "\n",
    "faerie_queen_lowered = faerie_queen.lower()\n",
    "venus_and_adonis_lowered = venus_and_adonis.lower()\n",
    "paradise_lost_lowered = paradise_lost.lower()\n",
    "finnagans_wake_lowered = finnagans_wake.lower()\n",
    "ulysses_lowered = ulysses.lower()\n",
    "\n",
    "# This tokenizes, removes punctuation, stop words. For the .csv I turned the lists of tokens into strings.\n",
    "\n",
    "baldus_tokens = word_tokenizer.tokenize(baldus_lowered)\n",
    "baldus_tokens_nopun = [token for token in baldus_tokens if token not in ['.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\"]]\n",
    "baldus_no_stops = [word for word in baldus_tokens_nopun if not word in STOPS_LIST]\n",
    "dererumnatura_tokens = word_tokenizer.tokenize(dererumnatura_lowered)\n",
    "dererumnatura_tokens_nopun = [token for token in dererumnatura_tokens if token not in ['.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\"]]\n",
    "dererumnatura_no_stops = [word for word in dererumnatura_tokens_nopun if not word in STOPS_LIST]\n",
    "dererumnatura_lemmas = lemmatizer.lemmatize(dererumnatura_no_stops)\n",
    "aeneid_tokens = word_tokenizer.tokenize(aeneid_lowered)\n",
    "aeneid_tokens_nopun = [token for token in aeneid_tokens if token not in ['.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\"]] \n",
    "aeneid_no_stops = [word for word in aeneid_tokens_nopun if not word in STOPS_LIST]\n",
    "thebaid_tokens = word_tokenizer.tokenize(thebaid_lowered)\n",
    "thebaid_tokens_nopun = [token for token in thebaid_tokens if token not in ['.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\"]]\n",
    "thebaid_no_stops = [word for word in thebaid_tokens_nopun if not word in STOPS_LIST]\n",
    "pharsalia_tokens = word_tokenizer.tokenize(pharsalia_lowered)\n",
    "pharsalia_tokens_nopun = [token for token in pharsalia_tokens if token not in ['.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\"]]\n",
    "pharsalia_no_stops = [word for word in pharsalia_tokens_nopun if not word in STOPS_LIST]\n",
    "africa_tokens = word_tokenizer.tokenize(africa_lowered)\n",
    "africa_tokens_nopun = [token for token in africa_tokens if token not in ['.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\"]]\n",
    "africa_no_stops = [word for word in africa_tokens_nopun if not word in STOPS_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lemmatizes the Italian texts with SpaCy and it_core_news_sm\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_sm\", disable=[\"ner\", \"textcat\"])\n",
    "nlp.max_length = 1500000\n",
    "\n",
    "commedia_tokens = lemmatize(commedia_lowered)\n",
    "commedia_tokens_nopun = [token for token in commedia_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "commedia_no_stops = [word for word in commedia_tokens_nopun if word not in stopwords.words('italian')]\n",
    "commedia_string_no_stops = \" \".join(commedia_no_stops)\n",
    "morgante_tokens = lemmatize(morgante_lowered)\n",
    "morgante_tokens_nopun = [token for token in morgante_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "morgante_no_stops = [word for word in morgante_tokens_nopun if word not in stopwords.words('italian')]\n",
    "morgante_string_no_stops = \" \".join(morgante_no_stops)\n",
    "orlando_innamorato_tokens = lemmatize(orlando_innamorato_lowered) \n",
    "orlando_innamorato_tokens_nopun = [token for token in orlando_innamorato_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "orlando_innamorato_no_stops = [word for word in orlando_innamorato_tokens_nopun if word not in stopwords.words('italian')]\n",
    "orlando_innamorato_string_no_stops = \" \".join(orlando_innamorato_no_stops)\n",
    "orlando_furioso_tokens = lemmatize(orlando_furioso_lowered) \n",
    "orlando_furioso_tokens_nopun = [token for token in orlando_furioso_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "orlando_furioso_no_stops = [word for word in orlando_furioso_tokens_nopun if word not in stopwords.words('italian')]\n",
    "orlando_furioso_string_no_stops = \" \".join(orlando_furioso_no_stops)\n",
    "italia_liberata_tokens = lemmatize(italia_liberata_lowered)\n",
    "italia_liberata_tokens_nopun = [token for token in italia_liberata_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "italia_liberata_no_stops = [word for word in italia_liberata_tokens_nopun if word not in stopwords.words('italian')]\n",
    "italia_liberata_string_no_stops = \" \".join(italia_liberata_no_stops)\n",
    "gerusalemme_liberata_tokens = lemmatize(gerusalemme_liberata_lowered)\n",
    "gerusalemme_liberata_tokens_nopun = [token for token in gerusalemme_liberata_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "gerusalemme_liberata_no_stops = [word for word in gerusalemme_liberata_tokens_nopun if word not in stopwords.words('italian')]\n",
    "gerusalemme_liberata_string_no_stops = \" \".join(gerusalemme_liberata_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This load the model en_core_web_sm and lemmatizes the English poems\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "nlp.max_length = 1600000\n",
    "\n",
    "faerie_queen_tokens = lemmatize(faerie_queen_lowered)\n",
    "faerie_queen_nopun = [token for token in faerie_queen_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "faerie_queen_no_stops = [word for word in faerie_queen_nopun if word not in stopwords.words('english')]\n",
    "faerie_queen_string_no_stops = \" \".join(faerie_queen_no_stops)\n",
    "venus_and_adonis_tokens = lemmatize(venus_and_adonis_lowered)\n",
    "venus_and_adonis_nopun = [token for token in venus_and_adonis_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "venus_and_adonis_no_stops = [word for word in venus_and_adonis_nopun if word not in stopwords.words('english')]\n",
    "venus_and_adonis_string_no_stops = \" \".join(venus_and_adonis_no_stops)\n",
    "finnagans_wake_tokens = lemmatize(finnagans_wake_lowered)\n",
    "finnagans_wake_nopun = [token for token in finnagans_wake_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "finnagans_wake_no_stops = [word for word in finnagans_wake_nopun if word not in stopwords.words('english')]\n",
    "finnagans_wake_string_no_stops = \" \".join(finnagans_wake_no_stops)\n",
    "ulysses_tokens = lemmatize(ulysses_lowered)\n",
    "ulysses_nopun = [token for token in ulysses_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "ulysses_no_stops = [word for word in ulysses_nopun if word not in stopwords.words('english')]\n",
    "ulysses_string_no_stops = \" \".join(ulysses_no_stops)\n",
    "paradise_lost_tokens = lemmatize(paradise_lost_lowered)\n",
    "paradise_lost_nopun = [token for token in paradise_lost_tokens if token not in [\"'\", '\\n\\n \\n\\n', '\\n\\n', '\\n', '.', ',', ':', ';', '—', '?', '!','\"', \"*\", \"„\", \")\", \"(\", \"-\", \"»\", \"«\"]]\n",
    "paradise_lost_no_stops = [word for word in paradise_lost_nopun if word not in stopwords.words('english')]\n",
    "paradise_lost_string_no_stops = \" \".join(paradise_lost_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a .csv\n",
    "\n",
    "title = [\"De Rerum Natura\", \"Aeneid\", \"Pharsalia\", \"Thebaid\", \"Africa\", \"Christiad\", \"De Partu Virginis\", \"Commedia\", \"Morgante,\" \"Orlando innamorato\", \"Orlando furioso\", \"Italia liberata\" \"Gerusalemme liberata\", \"Macaronea\", \"Ad Magnificus Dominus Gasparus Vescontus\", \"Baldus\", \"Eglogae\", \"Macaronea Contra Macaroneam Bassani\", \"Macaronea Contra Savoynos\", \"Moschaea\", \"Nobile Vigonze Opus\", \"Vergiliana\", \"Zanitonella\"]\n",
    "date = [\"99-55 BC\", \"29-19 BC\", \"61-65\", \"45-96\", \"1338-1374\", \"1535\", \"1526\", \"1321\", \"1478\", \"1483-1495\", \"1516-1532\", \"1547\", \"1581\", \"?\", \"?\", \"1552\", \"1517\", \"?\", \"?\", \"1521\", \"1502\", \"?\", \"1521\"]\n",
    "token_num = [len(set(dererumnatura_no_stops)), len(set(aeneid_no_stops)), len(set(pharsalia_no_stops)), len(set(thebaid_no_stops)), len(set(christiad_no_stops)), len(set(de_partu_virginis_no_stops)), len(set(commedia_no_stops)), len(set(morgante_no_stops)), len(set(orlando_innamorato_no_stops)), len(set(orlando_furioso_no_stops)), len(set(italia_liberata_no_stops)), len(set(gerusalemme_liberata_no_stops)), len(set(macaronea_no_stops)), len(set(magnificus_no_stops)), len(set(baldus_no_stops)), len(set(eglogae_no_stops)), len(set(contra_no_stops)), len(set(contra2_no_stops)), len(set(moschaea_no_stops)), len(set(vigonze_no_stops)), len(set(vergiliana_no_stops)), len(set(zanitonella_no_stops))]\n",
    "fulltext = [dererumnatura, aeneid, pharsalia, thebaid, africa, christiad, de_partu_virginis, commedia, morgante, orlando_innamorato, orlando_furioso, italia_liberata, gerusalemme_liberata, macaronea, magnificus, baldus, eglogae, contra, contra2, moschaea, vigonze, vergiliana, zanitonella]\n",
    "token_strings = [dererumnatura_string_no_stops, pharsalia_string_no_stops, thebaid_string_no_stops, africa_string_no_stops, christiad_string_no_stops, de_partu_virginis_string_no_stops, commedia_string_no_stops, morgante_string_no_stops, orlando_innamorato_string_no_stops, orlando_furioso_string_no_stops, italia_liberata_string_no_stops, gerusalemme_liberata_string_no_stops, macaronea_string_no_stops, magnificus_string_no_stops, baldus_string_no_stops, eglogae_string_no_stops, contra_string_no_stops, contra2_string_no_stops, moschaea_string_no_stops, vigonze_string_no_stops, vergiliana_string_no_stops, zanitonella_string_no_stops]\n",
    "lemmas = [dererumnatura_lemmas, pharsalia_lemmas, thebaid_lemmas, africa_lemmas, christiad_lemmas, de_partu_virginis_lemmas, commedia_no_stops, morgante_no_stops, orlando_innamorato_no_stops, orlando_furioso_no_stops, italia_liberata_no_stops, gerusalemme_liberata_no_stops, macaronea_lemmas, magnificus_lemmas, baldus_lemmas, eglogae_lemmas, contra_lemmas, contra2_lemmas, moschaea_lemmas, vigonze_lemmas, vergiliana_lemmas, zanitonella_lemmas]\n",
    "\n",
    "data = list(zip(title, date, token_num, fulltext, token_strings, lemmas))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"title\", \"date\", \"number of unique words\", \"full_text\", \"processed_tokens_string\", \"lemmas\"])\n",
    "df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Count Hapax Legonmena?\n",
    "\n",
    "I counted hapax legomena because this number can be stastically relevant for studying literary artifacts. It has been a heuristic for:\n",
    "\n",
    "1) <i>Authorship</i>. For example, W.P. Workman counted hapaxes in Shakespeares' works (Workman, H. P. “The Hapax Legomena of St. Paul.” ExpTim 7, 1896, 418–19).\n",
    "\n",
    "2) <i>Language</i>. All the Latin and MP texts in this study have a higher number of hapax than the Italian texts.\n",
    "\n",
    "2) <i>Corpus integrity</i> A high number can mean a corpus is too small or no longer exists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns a list of the hapax to a variable\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "dererumnatura_unique = FreqDist(dererumnatura_no_stops)\n",
    "dererumnatura_hapaxes = dererumnatura_unique.hapaxes()\n",
    "aeneid_unique = FreqDist(aeneid_no_stops)\n",
    "aeneid_hapaxes = aeneid_unique.hapaxes()\n",
    "thebaid_unique = FreqDist(thebaid_no_stops)\n",
    "thebaid_hapaxes = thebaid_unique.hapaxes()\n",
    "pharsalia_unique = FreqDist(pharsalia_no_stops)\n",
    "pharsalia_hapaxes = pharsalia_unique.hapaxes()\n",
    "africa_unique = FreqDist(africa_no_stops)\n",
    "africa_hapaxes = africa_unique.hapaxes()\n",
    "commedia_unique = FreqDist(commedia_no_stops)\n",
    "commedia_hapaxes = commedia_unique.hapaxes()\n",
    "morgante_unique = FreqDist(morgante_no_stops)\n",
    "morgante_hapaxes = morgante_unique.hapaxes()\n",
    "orlando_innamorato_unique = FreqDist(orlando_innamorato_no_stops)\n",
    "orlando_innamorato_hapaxes = orlando_innamorato_unique.hapaxes()\n",
    "orlando_furioso_unique = FreqDist(orlando_furioso_no_stops)\n",
    "orlando_furioso_hapaxes = orlando_furioso_unique.hapaxes()\n",
    "italia_liberata_unique = FreqDist(italia_liberata_no_stops)\n",
    "italia_liberata_hapaxes = italia_liberata_unique.hapaxes()\n",
    "gerusalemme_liberata_unique = FreqDist(gerusalemme_liberata_no_stops)\n",
    "gerusalemme_liberata_hapaxes = gerusalemme_liberata_unique.hapaxes()\n",
    "faerie_queen_unique = FreqDist(faerie_queen_no_stops)\n",
    "faerie_queen_hapaxes = faerie_queen_unique.hapaxes()\n",
    "venus_and_adonis_unique = FreqDist(venus_and_adonis_no_stops)\n",
    "venus_and_adonis_hapaxes = venus_and_adonis_unique.hapaxes()\n",
    "paradise_lost_unique = FreqDist(paradise_lost_no_stops)\n",
    "paradise_lost_hapaxes = paradise_lost_unique.hapaxes()\n",
    "finnagans_wake_unique = FreqDist(finnagans_wake_no_stops)\n",
    "finnagans_wake_hapaxes = finnagans_wake_unique.hapaxes()\n",
    "ulysses_unique = FreqDist(ulysses_no_stops)\n",
    "ulysses_hapaxes = ulysses_unique.hapaxes()\n",
    "baldus_unique = FreqDist(baldus_no_stops)\n",
    "baldus_hapaxes = baldus_unique.hapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a dataframe with columns for title, language, number of hapax, and total number of words\n",
    "\n",
    "t = [\"De Rerum Natura\", \"Aeneid\", \"Pharsalia\", \"Thebaid\", \"Africa\", \"Commedia\", \"Morgante\", \"L'Innamoramento de Orlando\", \"Orlando furioso\", \"L'Italia liberata dai Goti\", \"Gerusalemme liberata\", \"Faerie Queene\", \"Venus and Adonis\", \"Paradise Lost\", \"Ulysses\", \"Finnagan's Wake\", \"Baldus\",]\n",
    "l = [\"Latin\", \"Latin\", \"Latin\", \"Latin\", \"neo-Latin\", \"Italian\", \"Italian\", \"Italian\", \"Italian\", \"Italian\", \"Italian\", \"English\", \"English\", \"English\", \"English\", \"Macaronic\", \"Macaronic\"]\n",
    "h = [len(dererumnatura_hapaxes), len(aenied_hapaxes), len(pharsalia_hapaxes), len(thebaid_hapaxes), len(africa_hapaxes), len(commedia_hapaxes), len(morgante_hapaxes), len(orlando_innamorato_hapaxes), len(orlando_furioso_hapaxes), len(italia_liberata_hapaxes), len(gerusalemme_liberata_hapaxes), len(faerie_queen_hapaxes), len(venus_and_adonis_hapaxes), len(paradise_lost_hapaxes), len(ulysses_hapaxes), len(finnagans_wake_hapaxes), len(baldus_hapaxes)]\n",
    "w = [len(dererumnatura_tokens_nopun), len(aeneid_tokens_nopun), len(pharsalia_tokens_nopun), len(thebaid_tokens_nopun), len(africa_tokens_nopun), len(commedia_tokens_nopun), len(morgante_tokens_nopun), len(orlando_innamorato_tokens_nopun), len(orlando_furioso_tokens_nopun), len(italia_liberata_tokens_nopun), len(gerusalemme_liberata_tokens_nopun), len(faerie_queen_nopun), len(venus_and_adonis_nopun), len(paradise_lost_nopun), len(ulysses_nopun), len(finnagans_wake_nopun), len(baldus_tokens_nopun)]\n",
    "d = list(zip(t, l, h, w))\n",
    "df = pd.DataFrame(d, columns=[\"Title\", \"Language\", \"Hapax\", \"Words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c17580c79272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Makes a scatterplot of unique words vs. total words with dots classified by language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m g = sns.scatterplot(x='Hapax',\n\u001b[1;32m      5\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Makes a scatterplot of unique words vs. total words with dots classified by language\n",
    "\n",
    "sns.set()\n",
    "g = sns.scatterplot(x='Hapax',\n",
    "                y='Words',\n",
    "                hue=\"Language\", \n",
    "                style=\"Language\",\n",
    "                palette=\"Set1\",\n",
    "                data=df).set(title = 'Unique Words vs Total Words')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2)\n",
    "plt.savefig('Scatterplot.png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatterplot \n",
    "<img src=\"../Scatterplot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a barplot showing the total number of hapax in the poems, side by side\n",
    "\n",
    "sns.barplot(x='Hapax',\n",
    "            y='Title',\n",
    "            palette=\"Blues_d\",\n",
    "            data=df).set(title = 'Number of Unique Words');\n",
    "plt.savefig('Barplot.png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barchart\n",
    "<img src=\"../Barplot.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's law\n",
    "\n",
    "Noticing the higher number of rare words in MP, I made rank-frequency distribution charts that visualize the degree to which the corpus adheres to an ideal Zipfian regression plot. George Zipf discovered that the relative frequency of words is inversely proportional to their rank:  \n",
    "\n",
    "<img src=\"../zipf.png\">\n",
    "\n",
    "My charts are based on the famous regression chart included in George Zipf's classic 1949 book, <i>Human Behavior and The Principle of Least Effort</i>. Notice that Zipf tested his discovery on James Joyce's <i>Ulysses</i>, which has many unusual words:\n",
    "\n",
    "<img src=\"../zip.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a dataframe with _tokens variables for log log scatterplots\n",
    "\n",
    "counter = Counter(aeneid_tokens_nopun)\n",
    "df = pd \\\n",
    "    .DataFrame.from_dict(counter, orient='index') \\\n",
    "    .sort_values(0, ascending=False) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index':'word', 0:'count'})\n",
    "df.index = df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a column in the df with the rank frequency proportions\n",
    "\n",
    "df['proportion'] = df['count']/df['count'].sum()\n",
    "\n",
    "vocab_size, _ = df.shape\n",
    "n_tokens = df['count'].sum()\n",
    "\n",
    "print(f'Vocabulary size: {vocab_size:>8,}\\nNumber of tokens: {n_tokens:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_zipf(N, k, s=1):\n",
    "    return (1/k**s)/(np.sum(1/(np.arange(1, N+1)**s)))\n",
    "\n",
    "N = n_tokens\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(f'N={N:,}, k={10**i:<3}: {classic_zipf(N, 10**i):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorize_classic_zipf = np.vectorize(lambda x: classic_zipf(N, x))\n",
    "\n",
    "df['predicted_proportion'] = vectorize_classic_zipf(df.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import spacy\n",
    "import collections\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(df.index.values)\n",
    "y = np.log(df['predicted_proportion'] * n_tokens)\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = {\n",
    "        'title': 'Rank'\n",
    "    },\n",
    "    yaxis = {\n",
    "        'title': 'Frequency'\n",
    "    }\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(df.index.values)\n",
    "y = np.log(df['count'])\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers',\n",
    "    name = 'Actual',\n",
    ")\n",
    "\n",
    "x = np.log(df.index.values)\n",
    "y = np.log(df['predicted_proportion'] * n_tokens)\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers',\n",
    "    name = 'Ideal'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_layout(\n",
    "    title=\"Zanitonella\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    " \n",
    "<img src=\"../charts.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../finbal.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the week 5 code to loop through the files of macaronic poetry, turn the list of tokens into long string,\n",
    "# and make it a dataframe\n",
    "\n",
    "for_dataframe = {}\n",
    "\n",
    "for file in dir_path:\n",
    "    with open(\"/Users/matthewmason/cltk/corpus/\" + file, \"r\", encoding=\"utf-8\") as to_open:\n",
    "         for_dataframe[file] = to_open.read()\n",
    "            \n",
    "mpcorpus = (pd.DataFrame.from_dict(for_dataframe, \n",
    "                                       orient = \"index\")\n",
    "                .reset_index().rename(index = str, \n",
    "                                      columns = {\"index\": \"File\", 0: \"Text\"}))\n",
    "\n",
    "mpcorpus = (pd.DataFrame.from_dict(for_dataframe, \n",
    "                                       orient = \"index\")\n",
    "                .reset_index().rename(index = str, \n",
    "                                      columns = {\"index\": \"File\", 0: \"Text\"}))\n",
    "\n",
    "mpcorpus[\"Text_processed\"] = mpcorpus[\"Text\"].map(lambda x: re.sub('[.,:;—?!\"*„)(-]', '', x))\n",
    "\n",
    "mpcorpus[\"Text_processed\"] = mpcorpus[\"Text_processed\"].map(lambda x: x.lower())\n",
    "\n",
    "mpcorpus['Text_processed'] = mpcorpus['Text_processed'].str.replace('\\d+', '')\n",
    "\n",
    "list(mpcorpus[0:1][\"Text_processed\"])\n",
    "\n",
    "long_string = ','.join(list(mpcorpus[\"Text_processed\"].values))\n",
    "long_string\n",
    "\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "mptokens = word_tokenizer.tokenize(long_string)\n",
    "mp_nostops = [word for word in mptokens if not word in STOPS_LIST]\n",
    "\n",
    "counter = Counter(mp_nostops)\n",
    "df = pd \\\n",
    "    .DataFrame.from_dict(counter, orient='index') \\\n",
    "    .sort_values(0, ascending=False) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index':'word', 0:'count'})\n",
    "df.index = df.index + 1\n",
    "\n",
    "df['proportion'] = df['count']/df['count'].sum()\n",
    "df.head()\n",
    "\n",
    "vocab_size, _ = df.shape\n",
    "n_tokens = df['count'].sum()\n",
    "\n",
    "print(f'Vocabulary size: {vocab_size:>8,}\\nNumber of tokens: {n_tokens:,}')\n",
    "\n",
    "def classic_zipf(N, k, s=1):\n",
    "    return (1/k**s)/(np.sum(1/(np.arange(1, N+1)**s)))\n",
    "\n",
    "N = n_tokens\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(f'N={N:,}, k={10**i:<3}: {classic_zipf(N, 10**i):.4f}')\n",
    "    \n",
    "%%time\n",
    "vectorize_classic_zipf = np.vectorize(lambda x: classic_zipf(N, x))\n",
    "\n",
    "df['predicted_proportion'] = vectorize_classic_zipf(df.index.values)\n",
    "\n",
    "x = np.log(df.index.values)\n",
    "y = np.log(df['predicted_proportion'] * n_tokens)\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = {\n",
    "        'title': 'Rank'\n",
    "    },\n",
    "    yaxis = {\n",
    "        'title': 'Frequency'\n",
    "    }\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(df.index.values)\n",
    "y = np.log(df['count'])\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers',\n",
    "    name = 'Actual',\n",
    ")\n",
    "\n",
    "x = np.log(df.index.values)\n",
    "y = np.log(df['predicted_proportion'] * n_tokens)\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers',\n",
    "    name = 'Ideal'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_layout(\n",
    "    title=\"MP Corpus\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../MP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
